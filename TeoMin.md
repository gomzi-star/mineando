---


---

<h1 id="teorico-osea-digamos">Teorico ‘osea digamos’</h1>
<p><strong>Ley de Zipf:</strong><br>
Los lenguajes naturales (LN) tienen <em><strong>distribucion exponencial</strong></em></p>
<blockquote>
<p>Parametros parecidos para cualquier lenguaje!!!</p>
</blockquote>
<h2 id="analisis-del-lenguaje">Analisis del Lenguaje</h2>
<blockquote>
<p>Cancion del dia: <a href="https://open.spotify.com/intl-es/track/22Fvc0gkf7ZCNQEQ2Oxj6D?si=9a43ff5791a64cd9">Smoke Remix  - Blood Orange, Yves Tumor</a></p>
</blockquote>
<p>LN = Fenomeno Natural</p>
<blockquote>
<p>Sordos de Nicaragua<br>
Cambio del idioma persa (del nom-ac to erg-abs)<br>
‘ser’ es el valor por defecto de varias lenguas no hace falta la oalabra mr Heidegger</p>
</blockquote>
<p><em><strong>No podemos reducir solo a gramática!</strong></em><br>
Siguiendo al Noam:odemosobservar sitemaricidades trarables automaticamente.</p>
<ul>
<li>mayor parte del LN es <em>regular</em></li>
<li>otra es "independiente* del contexto</li>
<li>pequeña parte <em>sebsible</em> al contexto<br>
<mark>LN es oral</mark><br>
la escritura es un artefacto <em><strong>sobre</strong></em> LN<br>
<em><strong>Pasos:</strong></em></li>
<li>Comprender</li>
<li>Generar<br>
end-to-end vs analisis por niveles<br>
ejemplo trad. autimatica:</li>
<li>analisis y generacion</li>
<li>canal ruidoso</li>
<li>transformers</li>
</ul>
<p><a href="https://docs.google.com/presentation/d/1APyWCRzmQYnyPU1vHGfTF3TNrluGHepyFCV3JkVJBiQ/edit#slide=id.p"><em><strong>SLIDES USADOS</strong></em></a></p>
<h2 id="evaluacion-de-modelos">Evaluacion de Modelos</h2>
<blockquote>
<p>Cancion del dia: <a href="https://open.spotify.com/intl-es/track/4WiiRw2PHMNQE0ad6y6GdD?si=c42a4b6f2ea744a1">Chocolate - The 1975</a></p>
</blockquote>
<p>Principio de Coperacion<br>
<a href="https://en.wikipedia.org/wiki/Evaluation_of_binary_classifiers">Evaluation of binary classifiers</a><br>
<a href="https://en.wikipedia.org/wiki/Evaluation_of_binary_classifiers#Contingency_table">Contingency Table</a></p>
<ul>
<li><a href="https://en.wikipedia.org/wiki/Sensitivity_and_specificity">Sensitivity (Recall) and Specifity (not Sense and Sensibility)</a></li>
<li><a href="https://en.wikipedia.org/wiki/Positive_and_negative_predictive_values">Positive and negative predictive values</a></li>
<li><a href="https://en.wikipedia.org/wiki/Precision_and_recall">Precision and Recall</a>
<ul>
<li><a href="https://en.wikipedia.org/wiki/Precision_and_recall#F-measure">F-measure</a></li>
</ul>
</li>
</ul>
<p><a href="https://en.wikipedia.org/wiki/Confusion_matrix">Confusion Matrix</a><br>
<a href="https://en.wikipedia.org/wiki/Receiver_operating_characteristic#Basic_concept">ROC</a><br>
<a href="https://en.wikipedia.org/wiki/Cohen%27s_kappa">Kappa de Cohen</a></p>
<p>En contexto de NLP, para reducir el sesgo predictivo, podemos contrastar con variaciones del mimso lenguaje.</p>
<p><a href="https://en.wikipedia.org/wiki/Fairness_(machine_learning)#">Fairness</a></p>
<ul>
<li><a href="https://en.wikipedia.org/wiki/Fairness_(machine_learning)#Group_fairness_criteria">Group fairness criteria</a></li>
</ul>
<p><a href="https://nordicapis.com/7-ways-to-test-llms/?ref=dailydev">7 Ways to Test LLMs</a></p>
<p><a href="https://docs.google.com/presentation/d/10Zz1K8jx3Q5XOs78tlMGFfiTAKkfAXTTaea3KgDBQIk/edit#slide=id.gb0f795d4a0_0_20"><em><strong>SLIDES USADOS</strong></em></a></p>

